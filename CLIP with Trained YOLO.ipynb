{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Broiler and Laying Hen Detection and labeling\n",
    "import os\n",
    "import torch\n",
    "from clip import load as load_clip  # CLIP\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModelWithProjection, CLIPTextModelWithProjection\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = load_clip(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model = YOLO(\"/weights/best.pt\")  # Replace with your trained model\n",
    "\n",
    "# Input image directory and text prompts\n",
    "image_dir = \"/images/\"  # Directory containing images\n",
    "text_prompts = {\n",
    "    \"\": \"\",\n",
    "    \"\": \"\",\n",
    "}\n",
    "\n",
    "# Function to crop image using bounding box\n",
    "def crop_image(image, box):\n",
    "    x1, y1, x2, y2 = map(int, box[:4])  # YOLO box format: x1, y1, x2, y2\n",
    "    cropped = image.crop((x1, y1, x2, y2))\n",
    "    return preprocess(cropped).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode text features for both classes\n",
    "text_features = {}\n",
    "for label, prompt in text_prompts.items():\n",
    "    text_input = clip.tokenize([prompt]).to(device)  # Use the clip.tokenize method to properly tokenize the text\n",
    "    text_features[label] = clip_model.encode_text(text_input).detach()\n",
    "\n",
    "# Output directory for results\n",
    "output_dir = \"/Outputs/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for file_name in os.listdir(image_dir):\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "    if not file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue  # Skip non-image files\n",
    "\n",
    "    image = Image.open(file_path).convert(\"RGB\")  # Load and ensure RGB format\n",
    "    results = yolo_model.predict(source=file_path, conf=0.5, save=False)  # Run YOLO model\n",
    "    \n",
    "    # Debugging: Print detection results\n",
    "    print(f\"Processing {file_name}\")\n",
    "    print(\"Detection results:\", results)\n",
    "\n",
    "    # Filter YOLO detections and process each bounding box\n",
    "    output_boxes = []\n",
    "    for detection in results[0].boxes:  # Access the first image's results\n",
    "        box = detection.xyxy[0].cpu().numpy()  # Bounding box coordinates\n",
    "        cls_id = int(detection.cls[0])  # Class ID\n",
    "        class_name = yolo_model.names[cls_id]  # Get class name\n",
    "\n",
    "        print(f\"Detected: {class_name} with confidence {detection.conf[0]}\")  # Debugging print\n",
    "        \n",
    "        if class_name in text_features:  # Check if the detected class is Broiler or Hen\n",
    "            cropped_region = crop_image(image, box)  # Crop detected region\n",
    "            cropped_region_features = clip_model.encode_image(cropped_region).detach()\n",
    "            \n",
    "            # Compute similarity for the specific class\n",
    "            similarity = torch.nn.functional.cosine_similarity(\n",
    "                text_features[class_name], cropped_region_features\n",
    "            )\n",
    "            print(f\"Similarity score for {class_name}: {similarity.item()}\")  # Debugging print\n",
    "            if similarity.item() > 0.5:  # Threshold for similarity\n",
    "                output_boxes.append((box, class_name, similarity.item()))\n",
    "\n",
    "    # Draw bounding boxes and save results\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label, score in output_boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "        draw.text((x1, y1 - 10), f\"{label}: {score:.2f}\", fill=\"red\")\n",
    "\n",
    "    # Save the output image\n",
    "    output_path = os.path.join(output_dir, f\"detection_{file_name}\")\n",
    "    image.save(output_path)\n",
    "    print(f\"Detection results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d839e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLIP with YOLO models final model with behavior labels and images output\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import clip\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model = YOLO(\"/weights/best.pt\")  # Replace with your trained model\n",
    "\n",
    "# Input image directory and text prompts\n",
    "image_dir = \"/images/\"  # Directory containing images\n",
    "text_prompts = {\n",
    "    \"\": \"\",\n",
    "    \"\": \"\",\n",
    "    \"\": \"\",\n",
    "    \"\": \"\",\n",
    "}\n",
    "\n",
    "# Function to crop image using bounding box\n",
    "def crop_image(image, box):\n",
    "    x1, y1, x2, y2 = map(int, box[:4])  # YOLO box format: x1, y1, x2, y2\n",
    "    cropped = image.crop((x1, y1, x2, y2))\n",
    "    return preprocess(cropped).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode text features for both classes\n",
    "text_features = {}\n",
    "for label, prompt in text_prompts.items():\n",
    "    text_input = clip.tokenize([prompt]).to(device)  # Tokenize the prompt\n",
    "    text_features[label] = clip_model.encode_text(text_input).detach()\n",
    "\n",
    "# Output directory for results\n",
    "output_dir = \"/Predictedimages/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Labels directory for YOLO format labels\n",
    "labels_dir = \"/Predictedlabels/\"\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the directory\n",
    "for file_name in os.listdir(image_dir):\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "    if not file_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        continue  # Skip non-image files\n",
    "\n",
    "    image = Image.open(file_path).convert(\"RGB\")  # Load and ensure RGB format\n",
    "    results = yolo_model.predict(source=file_path, conf=0.5, save=True)  # Lower confidence threshold\n",
    "    \n",
    "    print(f\"Processing {file_name}\")\n",
    "    print(\"Detection results:\", results)  # Debugging print\n",
    "    \n",
    "    # Check if results are empty\n",
    "    if not results[0].boxes:\n",
    "        print(f\"No detections for {file_name}\")\n",
    "        continue  # Skip image if no detections\n",
    "    \n",
    "    # Filter YOLO detections and process each bounding box\n",
    "    output_boxes = []\n",
    "    for detection in results[0].boxes:  # Access the first image's results\n",
    "        box = detection.xyxy[0].cpu().numpy()  # Bounding box coordinates\n",
    "        cls_id = int(detection.cls[0])  # Class ID\n",
    "        class_name = yolo_model.names[cls_id]  # Get class name\n",
    "\n",
    "        print(f\"Detected: {class_name} with confidence {detection.conf[0]}\")  # Debugging print\n",
    "        \n",
    "        # Check if the detected class name matches any of the text prompts\n",
    "        matching_label = None\n",
    "        for label, prompt in text_prompts.items():\n",
    "            if prompt.lower() in class_name.lower():  # Case-insensitive match\n",
    "                matching_label = label\n",
    "                break\n",
    "\n",
    "        if matching_label:  # If a match is found for the label\n",
    "            cropped_region = crop_image(image, box)  # Crop detected region\n",
    "            cropped_region_features = clip_model.encode_image(cropped_region).detach()\n",
    "            \n",
    "            # Compute similarity for the specific class\n",
    "            similarity = torch.nn.functional.cosine_similarity(\n",
    "                text_features[matching_label], cropped_region_features\n",
    "            )\n",
    "            print(f\"Similarity score for {matching_label}: {similarity.item()}\")  # Debugging print\n",
    "            if similarity.item() > 0.2:  # Lower the similarity threshold\n",
    "                output_boxes.append((box, matching_label, similarity.item()))\n",
    "\n",
    "    # Draw bounding boxes and save results\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    yolo_labels = []  # To store YOLO formatted labels\n",
    "\n",
    "    for box, label, score in output_boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"blue\", width=3)\n",
    "        draw.text((x1, y1 - 10), f\"{label}: {score:.2f}\", fill=\"blue\")\n",
    "\n",
    "        # Convert to YOLO format: <class_id> <x_center> <y_center> <width> <height>\n",
    "        x_center = (x1 + x2) / 2 / image.width\n",
    "        y_center = (y1 + y2) / 2 / image.height\n",
    "        width = (x2 - x1) / image.width\n",
    "        height = (y2 - y1) / image.height\n",
    "\n",
    "        # Get the class ID from the class name\n",
    "        cls_id = list(text_prompts.keys()).index(label)  # Get the class ID from the text_prompts\n",
    "\n",
    "        # Append to YOLO label list\n",
    "        yolo_labels.append(f\"{cls_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "    # Show the image for debugging\n",
    "    image.show()\n",
    "\n",
    "    # Save the output image\n",
    "    output_path = os.path.join(output_dir, f\"detection_{file_name}\")\n",
    "    image.save(output_path)\n",
    "    print(f\"Detection results saved to {output_path}\")\n",
    "\n",
    "    # Save YOLO format labels to a text file\n",
    "    label_path = os.path.join(labels_dir, f\"{os.path.splitext(file_name)[0]}.txt\")\n",
    "    with open(label_path, 'w') as label_file:\n",
    "        for label in yolo_labels:\n",
    "            label_file.write(label + \"\\n\")\n",
    "    print(f\"YOLO labels saved to {label_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
